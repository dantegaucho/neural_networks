{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232785d9",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "#### Understanding Neural Networks and Tensors\n",
    "Neural networks are computational models inspired by the structure of the human brain. They consist of layers of interconnected units called neurons, organized into an input layer, one or more hidden layers, and an output layer. These networks are designed to learn patterns in data by adjusting the weights of connections between neurons through a process called training. The hidden layers are especially important, as they transform the input data using weighted sums and activation functions to capture complex relationships and features.\n",
    "\n",
    "In deep learning, data is represented using tensors, which are multi-dimensional arrays. A tensor can take various forms: a scalar (0D), a vector (1D), a matrix (2D), or higher-dimensional arrays (3D and beyond). For instance, images are commonly represented as 3D tensors—height, width, and color channels (e.g., RGB). So, a 64x64 pixel image with 3 channels would be a tensor of shape [64, 64, 3]. Tensors are foundational to how data flows through a neural network and are essential to model computation.\n",
    "\n",
    "![alt text](image1.png)\n",
    "\n",
    "#### Forward and Backward Propagation\n",
    "`Forward` and `backward` **propagation** are the core processes that enable a neural network to learn. In forward propagation, input data (represented as tensors) flows through the network layer by layer. Each neuron in a layer computes a weighted sum of its inputs, applies an activation function (like ReLU or sigmoid), and passes the result to the next layer. This continues until the network produces an output—such as a class label in image classification.\n",
    "\n",
    "Once the output is compared to the true label using a loss function, the network uses backward propagation to update its internal parameters (weights and biases). Backpropagation works by computing gradients of the loss with respect to each weight using the chain rule of calculus. These gradients indicate how much each weight contributed to the error, allowing the model to adjust the weights in the direction that minimizes the loss. This optimization is typically done using gradient descent or a variant like Adam. Through many iterations of forward and backward passes, the network gradually improves its performance.\n",
    "\n",
    "#### Tensor Operations in Neural Networks\n",
    "Neural networks rely heavily on tensor operations to process and transform data. Common operations include element-wise addition and multiplication, matrix multiplication, and broadcasting, which allows operations on tensors of different shapes. For example, when data flows through a layer, it’s multiplied by a weight matrix and added to a bias vector—both of which are tensors. These operations are performed efficiently using libraries like TensorFlow or PyTorch, which are optimized for hardware acceleration (like GPUs). Understanding how these operations work at the tensor level is crucial to grasp how information propagates and is transformed in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e9913",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66460e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "variety",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pricing",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sales",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d7bba250-a269-40ee-a838-7a15f1a5cab8",
       "rows": [
        [
         "0",
         "strawberry",
         "3.396088185477276",
         "mall",
         "1"
        ],
        [
         "1",
         "chocolate",
         "2.82322794156873",
         "park",
         "0"
        ],
        [
         "2",
         "strawberry",
         "1.5851360000154462",
         "mall",
         "0"
        ],
        [
         "3",
         "strawberry",
         "3.437354513972435",
         "park",
         "0"
        ],
        [
         "4",
         "chocolate",
         "2.7441895420628213",
         "beach",
         "0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variety</th>\n",
       "      <th>pricing</th>\n",
       "      <th>location</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>3.396088</td>\n",
       "      <td>mall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>2.823228</td>\n",
       "      <td>park</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>1.585136</td>\n",
       "      <td>mall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>3.437355</td>\n",
       "      <td>park</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>2.744190</td>\n",
       "      <td>beach</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      variety   pricing location  sales\n",
       "0  strawberry  3.396088     mall      1\n",
       "1   chocolate  2.823228     park      0\n",
       "2  strawberry  1.585136     mall      0\n",
       "3  strawberry  3.437355     park      0\n",
       "4   chocolate  2.744190    beach      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./vendors.csv')\n",
    "\n",
    "data.head()\n",
    "# data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9909f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a208fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "#sequential\n",
    "model = models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe554081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the layers\n",
    "data_encoded = pd.get_dummies(data[['variety', 'location']], drop_first=True) #one-hot encoding\n",
    "\n",
    "\n",
    "data_encoded['location_mall'] = data_encoded['location_mall'].astype(int)\n",
    "data_encoded['location_park'] = data_encoded['location_park'].astype(int)\n",
    "data_encoded['variety_vanilla'] = data_encoded['variety_vanilla'].astype(int)\n",
    "data_encoded['variety_strawberry'] = data_encoded['variety_strawberry'].astype(int)\n",
    "\n",
    "X = pd.concat([data['pricing'], data_encoded], axis=1)\n",
    "y = data['sales'].astype(int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state= 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2ed883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.6383 - loss: 0.6807 - val_accuracy: 0.7986 - val_loss: 0.6226\n",
      "Epoch 2/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7964 - loss: 0.6029 - val_accuracy: 0.7986 - val_loss: 0.5322\n",
      "Epoch 3/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7927 - loss: 0.5183 - val_accuracy: 0.8021 - val_loss: 0.4825\n",
      "Epoch 4/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8114 - loss: 0.4750 - val_accuracy: 0.8071 - val_loss: 0.4755\n",
      "Epoch 5/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8062 - loss: 0.4752 - val_accuracy: 0.8186 - val_loss: 0.4665\n",
      "Epoch 6/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8144 - loss: 0.4681 - val_accuracy: 0.8214 - val_loss: 0.4579\n",
      "Epoch 7/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8133 - loss: 0.4675 - val_accuracy: 0.8207 - val_loss: 0.4578\n",
      "Epoch 8/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8127 - loss: 0.4637 - val_accuracy: 0.8207 - val_loss: 0.4546\n",
      "Epoch 9/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8172 - loss: 0.4598 - val_accuracy: 0.8200 - val_loss: 0.4555\n",
      "Epoch 10/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8177 - loss: 0.4587 - val_accuracy: 0.8193 - val_loss: 0.4542\n",
      "Epoch 11/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8229 - loss: 0.4525 - val_accuracy: 0.8236 - val_loss: 0.4537\n",
      "Epoch 12/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8205 - loss: 0.4571 - val_accuracy: 0.8236 - val_loss: 0.4531\n",
      "Epoch 13/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8152 - loss: 0.4643 - val_accuracy: 0.8236 - val_loss: 0.4525\n",
      "Epoch 14/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8190 - loss: 0.4574 - val_accuracy: 0.8207 - val_loss: 0.4521\n",
      "Epoch 15/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8160 - loss: 0.4605 - val_accuracy: 0.8193 - val_loss: 0.4526\n",
      "Epoch 16/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8113 - loss: 0.4665 - val_accuracy: 0.8243 - val_loss: 0.4526\n",
      "Epoch 17/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8176 - loss: 0.4597 - val_accuracy: 0.8236 - val_loss: 0.4511\n",
      "Epoch 18/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8173 - loss: 0.4610 - val_accuracy: 0.8207 - val_loss: 0.4520\n",
      "Epoch 19/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8257 - loss: 0.4493 - val_accuracy: 0.8193 - val_loss: 0.4546\n",
      "Epoch 20/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8164 - loss: 0.4599 - val_accuracy: 0.8221 - val_loss: 0.4544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20726ea8750>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##building a neural networks\n",
    "model.add(layers.Dense(18, activation='relu')) #hidden layer 1\n",
    "\n",
    "model.add(layers.Dense(9, activation='relu'))  #hidden layer 2\n",
    "\n",
    "model.add(layers.Dense(3, activation='relu'))  #hidden layer 3\n",
    "\n",
    "#output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#compiling your model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), metrics=['accuracy'], loss='binary_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=200, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e9e7e",
   "metadata": {},
   "source": [
    "### Deep Neural networks\n",
    "\n",
    "\n",
    "![alt text](image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248d194",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Understanding Gradient Descent\n",
    "- Activation functions at the hidden layers\n",
    "- Loss functions at the output layers\n",
    "- Tuning neural networks\n",
    "    - Tuning with regularization\n",
    "    - Tuning with normalization\n",
    "- Model Interpretability\n",
    "    - White-box model(decision tree)\n",
    "    - Black-box model(neural networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca434b",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "These are gatekeepers at each neuron that make sure to add non-linearity to the network.\n",
    "\n",
    "Here are examples of activation functions:\n",
    "- `ReLu` - if the output is positive then its not changed, however, if its negative its replaced with zero(`0`)\n",
    "- `sigmoid`- often used in binary classification(0-1)\n",
    "- `TanH` - outputs values between -1 and 1\n",
    "\n",
    "### Adjusting weights \n",
    "Mathematically:\n",
    "new weight = old weight - learning rate * gradient\n",
    "\n",
    "### Gradient Descent Algorithms\n",
    "1. **Adam(Adaptive Movement Estimation)** - Adjusts dynamically based on the steepness of the slope, so big steps for steeper slopes and small steps for shallow slopes.\n",
    "2. **SGD( Stochastic Gradient Descent)** - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb845fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b0b34",
   "metadata": {},
   "source": [
    "### Tuning a neural Network\n",
    "- Scaling of your features(use standard scaler or  min max scaler)\n",
    "- Regularization\n",
    "    * **L1(Lasso)** - this one can adjust weights to exactly zero(feature selection.)\n",
    "    * **L2(Ridge)** - this one brings the weights close to zero but never zero.\n",
    "    * **Dropout** - during training this technique randomly 'turns off' a certain percentage of neurons in a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3300603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7284 - loss: 0.6263 - val_accuracy: 0.7400 - val_loss: 0.5738\n",
      "Epoch 2/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7390 - loss: 0.5756 - val_accuracy: 0.7400 - val_loss: 0.5731\n",
      "Epoch 3/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7371 - loss: 0.5768 - val_accuracy: 0.7400 - val_loss: 0.5728\n",
      "Epoch 4/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7354 - loss: 0.5774 - val_accuracy: 0.7400 - val_loss: 0.5674\n",
      "Epoch 5/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7305 - loss: 0.5742 - val_accuracy: 0.7400 - val_loss: 0.5439\n",
      "Epoch 6/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7408 - loss: 0.5414 - val_accuracy: 0.8021 - val_loss: 0.5102\n",
      "Epoch 7/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8063 - loss: 0.5259 - val_accuracy: 0.8243 - val_loss: 0.4856\n",
      "Epoch 8/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8077 - loss: 0.5059 - val_accuracy: 0.8179 - val_loss: 0.4865\n",
      "Epoch 9/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8085 - loss: 0.4984 - val_accuracy: 0.8243 - val_loss: 0.4657\n",
      "Epoch 10/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8129 - loss: 0.4784 - val_accuracy: 0.8136 - val_loss: 0.4825\n",
      "Epoch 11/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8114 - loss: 0.4763 - val_accuracy: 0.8243 - val_loss: 0.4612\n",
      "Epoch 12/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8172 - loss: 0.4695 - val_accuracy: 0.8221 - val_loss: 0.4569\n",
      "Epoch 13/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8232 - loss: 0.4568 - val_accuracy: 0.8193 - val_loss: 0.4625\n",
      "Epoch 14/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8121 - loss: 0.4731 - val_accuracy: 0.8157 - val_loss: 0.4628\n",
      "Epoch 15/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8117 - loss: 0.4781 - val_accuracy: 0.8193 - val_loss: 0.4679\n",
      "Epoch 16/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.4721 - val_accuracy: 0.8136 - val_loss: 0.4740\n",
      "Epoch 17/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8219 - loss: 0.4643 - val_accuracy: 0.8236 - val_loss: 0.4548\n",
      "Epoch 18/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8111 - loss: 0.4735 - val_accuracy: 0.8243 - val_loss: 0.4526\n",
      "Epoch 19/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8176 - loss: 0.4665 - val_accuracy: 0.8221 - val_loss: 0.4585\n",
      "Epoch 20/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8160 - loss: 0.4663 - val_accuracy: 0.8236 - val_loss: 0.4518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20727f18750>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "##building a neural networks\n",
    "model.add(layers.Dense(18, activation='relu')) #hidden layer 1\n",
    "\n",
    "model.add(layers.Dropout(0.3))  #drop 30% of the neurons \n",
    "\n",
    "\n",
    "\n",
    "model.add(layers.Dense(9, activation='relu'))  #hidden layer 2\n",
    "\n",
    "model.add(layers.Dense(3, activation='relu'))  #hidden layer 3\n",
    "\n",
    "#output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#compiling your model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), metrics=['accuracy'], loss='binary_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=200, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce06aa6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720a826b",
   "metadata": {},
   "source": [
    "### Model Interpretability\n",
    "- **White box models** - these are models where the decision-making within the model is transparent and easy to understand.\n",
    "\n",
    "- **Black box models** - these are opaque and less intuitive in interpretation. Their internal workins are complex and not easy to understand. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('first_classification.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140664a",
   "metadata": {},
   "source": [
    "Here is the link to the tom&jerry image dataset. [Click here](https://www.kaggle.com/datasets/balabaskar/tom-and-jerry-image-classification/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0ddcc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
