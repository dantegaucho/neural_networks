{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232785d9",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "#### Understanding Neural Networks and Tensors\n",
    "Neural networks are computational models inspired by the structure of the human brain. They consist of layers of interconnected units called neurons, organized into an input layer, one or more hidden layers, and an output layer. These networks are designed to learn patterns in data by adjusting the weights of connections between neurons through a process called training. The hidden layers are especially important, as they transform the input data using weighted sums and activation functions to capture complex relationships and features.\n",
    "\n",
    "In deep learning, data is represented using tensors, which are multi-dimensional arrays. A tensor can take various forms: a scalar (0D), a vector (1D), a matrix (2D), or higher-dimensional arrays (3D and beyond). For instance, images are commonly represented as 3D tensors—height, width, and color channels (e.g., RGB). So, a 64x64 pixel image with 3 channels would be a tensor of shape [64, 64, 3]. Tensors are foundational to how data flows through a neural network and are essential to model computation.\n",
    "\n",
    "![alt text](image1.png)\n",
    "\n",
    "#### Forward and Backward Propagation\n",
    "`Forward` and `backward` **propagation** are the core processes that enable a neural network to learn. In forward propagation, input data (represented as tensors) flows through the network layer by layer. Each neuron in a layer computes a weighted sum of its inputs, applies an activation function (like ReLU or sigmoid), and passes the result to the next layer. This continues until the network produces an output—such as a class label in image classification.\n",
    "\n",
    "Once the output is compared to the true label using a loss function, the network uses backward propagation to update its internal parameters (weights and biases). Backpropagation works by computing gradients of the loss with respect to each weight using the chain rule of calculus. These gradients indicate how much each weight contributed to the error, allowing the model to adjust the weights in the direction that minimizes the loss. This optimization is typically done using gradient descent or a variant like Adam. Through many iterations of forward and backward passes, the network gradually improves its performance.\n",
    "\n",
    "#### Tensor Operations in Neural Networks\n",
    "Neural networks rely heavily on tensor operations to process and transform data. Common operations include element-wise addition and multiplication, matrix multiplication, and broadcasting, which allows operations on tensors of different shapes. For example, when data flows through a layer, it’s multiplied by a weight matrix and added to a bias vector—both of which are tensors. These operations are performed efficiently using libraries like TensorFlow or PyTorch, which are optimized for hardware acceleration (like GPUs). Understanding how these operations work at the tensor level is crucial to grasp how information propagates and is transformed in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66460e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5e9e7e",
   "metadata": {},
   "source": [
    "### Deep Neural networks\n",
    "\n",
    "\n",
    "![alt text](image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248d194",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xBase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
